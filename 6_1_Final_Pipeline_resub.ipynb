{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GvyAlJPhY8g"
   },
   "source": [
    "### 1. Loading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Ch6DNxk12-GZ",
    "outputId": "790985d6-f272-40ba-d6aa-4569f74cbccb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnJLpBez3D06"
   },
   "outputs": [],
   "source": [
    "# 1. Acquiring preprocessed_dataset\n",
    "tbs_df = pd.read_csv('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/tbs_df.csv')\n",
    "tbs_df = tbs_df.fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZncfF2dFZgG2"
   },
   "source": [
    "### 2. Tag Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4cIHkCHoIWO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow.keras.layers import Input, Softmax, GRU, LSTM, RNN, Embedding, Dense, RepeatVector, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ueRszMDo67Z"
   },
   "outputs": [],
   "source": [
    "# 1. Loading saved tokenizers\n",
    "import pickle\n",
    "handle = open('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/tag_predictor_tokenizer.pickle', 'rb')\n",
    "token_text = pickle.load(handle)\n",
    "\n",
    "handle = open('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/tag_predictor_token_tar.pickle', 'rb')\n",
    "token_tar = pickle.load(handle)\n",
    "\n",
    "text_vocab = token_text.word_index\n",
    "tar_vocab = token_tar.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "chUMwyD1pGE1",
    "outputId": "b04fe12e-2e2a-4316-8f5c-2c41e980c0e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117779"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. loading saved w2v model\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model_sg = Word2Vec.load(\"/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/word2vec_sg.model\")\n",
    "len(w2v_model_sg.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "lNKTFNfXoe4B",
    "outputId": "43d5fdf3-70fe-4532-955f-0432bb32e5ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. creating Embedding Matrix with Word2Vec representations\n",
    "max_words = 20000\n",
    "w2v_vocab = set(w2v_model_sg.wv.vocab)\n",
    "embedding_matrix = np.random.normal(loc = 0, scale = 0.15, size = (max_words+1, 100))\n",
    "for word, i in text_vocab.items():\n",
    "    if i <= max_words and word in w2v_vocab:\n",
    "      vector = w2v_model_sg[word]\n",
    "    # if vector is not None:\n",
    "      embedding_matrix[i] = vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_fzwmPVoZ3x"
   },
   "outputs": [],
   "source": [
    "# 4. Creating freezed 'Embedding layer'\n",
    "from tensorflow.keras.initializers import Constant\n",
    "text_embedding_layer = Embedding(input_dim = max_words+1, output_dim= 100, embeddings_initializer = Constant(embedding_matrix),\n",
    "                               mask_zero = True, trainable = False, name = 'text_embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "sT7qD_j9ntjG",
    "outputId": "67b693a5-741b-4804-d6d0-572e04efd8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_seq (InputLayer)        [(None, 250)]             0         \n",
      "_________________________________________________________________\n",
      "text_embed (Embedding)       (None, 250, 100)          2000100   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               176640    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "DECODER (GRU)                [(None, 5, 256), (None, 2 394752    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5, 243)            62451     \n",
      "=================================================================\n",
      "Total params: 2,633,943\n",
      "Trainable params: 633,843\n",
      "Non-trainable params: 2,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 5. Constructing a model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "enc_inputs = Input(name = 'text_seq', shape = (250,))\n",
    "enc_embed = text_embedding_layer(enc_inputs)\n",
    "encoder = Bidirectional(GRU(name = 'ENCODER', units = 128, dropout = 0.2))\n",
    "\n",
    "enc_out = encoder(enc_embed)\n",
    "\n",
    "dec_lstm = GRU(name = 'DECODER', units = 256, dropout = 0.2, return_sequences= True, return_state= True)\n",
    "\n",
    "repeat = RepeatVector(5)(enc_out)\n",
    "dec_out, dec_hidden = dec_lstm(repeat)\n",
    "\n",
    "dec_dense = Dense(units = len(tar_vocab)+1, activation = 'softmax')\n",
    "out = dec_dense(dec_out)\n",
    "\n",
    "model = Model(inputs = enc_inputs, outputs = out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyoXs42mqQ64"
   },
   "outputs": [],
   "source": [
    "# 6. loading model weights\n",
    "model.load_weights('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/weights--019--2.4615.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "MFcH-kVp4QoK",
    "outputId": "095bdf8f-2e01-4f39-88ff-437e29e50299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# defining a function to remove stop_words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('would')\n",
    "stop_words.update([chr(c) for c in range(97, 123)])\n",
    "# stop_words.remove('no'); stop_words.remove('not'); stop_words.remove('nor')\n",
    "\n",
    "def stopwrd_removal(sent):\n",
    "  lst = []\n",
    "  for wrd in sent.split():\n",
    "    if wrd not in stop_words:\n",
    "      lst.append(wrd)\n",
    "  return \" \".join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpXT83kuUnwL"
   },
   "outputs": [],
   "source": [
    "# 7. input text preprocessor\n",
    "def text_preprocessor(corpus, stop_word = False, remove_digits = False):\n",
    "  clean_corpus = []\n",
    "  for doc in corpus:\n",
    "    # 1. remove html tags, html urls, replace html comparison operators\n",
    "    clean_str = re.sub('<.*?>', '', doc)\n",
    "    clean_str = clean_str.replace('&lt;', '<')\\\n",
    "                .replace('&gt;', '>')\\\n",
    "                .replace('&le;', '<=' )\\\n",
    "                .replace('&ge;', '>=')\n",
    "\n",
    "    # 2. remove latex i,e., mostly formulas since it's mathematics based dataset\n",
    "    clean_str = re.sub('\\$.*?\\$', '', clean_str)\n",
    "\n",
    "    # 3. all lowercase \n",
    "    clean_str = clean_str.lower()\n",
    "\n",
    "    # 4. decontractions\n",
    "    clean_str = clean_str.replace(\"won't\", \"will not\").replace(\"can\\'t\", \"can not\").replace(\"n\\'t\", \" not\").replace(\"\\'re\", \" are\").\\\n",
    "                                                  replace(\"\\'s\", \" is\").replace(\"\\'d\", \" would\").replace(\"\\'ll\", \" will\").\\\n",
    "                                                  replace(\"\\'t\", \" not\").replace(\"\\'ve\", \" have\").replace(\"\\'m\", \" am\")\n",
    "\n",
    "    # 5. remove all special-characters other than alpha-numericals\n",
    "    clean_str = re.sub('\\W', ' ', clean_str)\n",
    "    if remove_digits == True:\n",
    "      clean_str = re.sub('\\d', ' ', clean_str)\n",
    "\n",
    "    # 6. Stop_word removal\n",
    "    if stop_word == True:\n",
    "      clean_str = stopwrd_removal(clean_str)\n",
    "\n",
    "    # 7. remove all white-space i.e., \\n, \\t, and extra_spaces\n",
    "    clean_str = re.sub('  +', ' ', clean_str)\n",
    "    clean_str = clean_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "\n",
    "    clean_corpus.append(clean_str)\n",
    "\n",
    "  return clean_corpus\n",
    "\n",
    "def padded_sequence(clean_corpus):\n",
    "    # 8. converting words into tokens (int)\n",
    "    tokens = token_text.texts_to_sequences(clean_corpus)\n",
    "\n",
    "    # 9. padding the sequence\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    sequence = pad_sequences(tokens, maxlen = 250, padding = 'post')\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57sWlOj8nhAz"
   },
   "outputs": [],
   "source": [
    "tar_vocab_reverse = {v:k for k,v in tar_vocab.items()}\n",
    "def final_tag_prediction(corpus):\n",
    "  \"\"\"\n",
    "  1. Text preprocessing on corpus\n",
    "  2. Convert clean corpus to padded sequence\n",
    "  3. Passes sequence through model and model generates output\n",
    "  4. Converts model output into tags list of tags\n",
    "  \"\"\"\n",
    "  # 10. model prediction\n",
    "  clean_corpus = text_preprocessor(corpus,  stop_word = False, remove_digits = False)\n",
    "  sequence = padded_sequence(clean_corpus)\n",
    "  model_out = model.predict(sequence)\n",
    "\n",
    "  # 11. converting model prediction to human readable tags\n",
    "  final_lst = []\n",
    "  for dp in model_out:\n",
    "    tar_wrd_idx_lst = []\n",
    "    for time_step in dp:\n",
    "      tar_wrd_idx = np.argmax(time_step)\n",
    "      tar_wrd = ('<' + tar_vocab_reverse[tar_wrd_idx] + '>')\n",
    "      tar_wrd_idx_lst.append(tar_wrd)\n",
    "      tar_wrd_idx_lst = list(set(tar_wrd_idx_lst))\n",
    "    final_lst.append(tar_wrd_idx_lst)\n",
    "\n",
    "  return final_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z55C__1wW28y"
   },
   "outputs": [],
   "source": [
    "# 12. Creating a dictionary of {tag : datapoint_idx}\n",
    "idx = []\n",
    "unique_tags = list(set(np.concatenate([tbs_df['tag_pred1'].unique(), tbs_df['tag_pred2'].unique(), tbs_df['tag_pred3'].unique(), tbs_df['tag_pred4'].unique(), tbs_df['tag_pred5'].unique()], axis = 0)))\n",
    "unique_tags.remove('-')\n",
    "for tag in unique_tags:\n",
    "  idx.append(list(tbs_df[(tbs_df['tag_pred1'] == tag) | (tbs_df['tag_pred2'] == tag) | (tbs_df['tag_pred3'] == tag) | (tbs_df['tag_pred4'] == tag) | (tbs_df['tag_pred5'] == tag)].index))\n",
    "tag_idx_dict = dict(zip(unique_tags, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQX9NJKSJprl"
   },
   "outputs": [],
   "source": [
    "# 13. final indices of tag_corpus\n",
    "def tag_corpus(tags_pred):\n",
    "  \"\"\"This function takes predicted tags and returns indices of corrospoinding questions from dataset\"\"\"\n",
    "  tag_corpus_idx = []\n",
    "  for tag in tags_pred:\n",
    "    tag_corpus_idx += tag_idx_dict[tag]\n",
    "  return list(set(tag_corpus_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "j72ULW4tKkOD",
    "outputId": "24c1a475-2c2a-4576-8552-e6a9276a3289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807 [['<outliers>']]\n",
      "CPU times: user 32.6 ms, sys: 2.98 ms, total: 35.6 ms\n",
      "Wall time: 34.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FINAL TESTING\n",
    "corpus = [tbs_df['Title'][129792]]\n",
    "tags_pred = final_tag_prediction(corpus)\n",
    "tag_corpus_idx = tag_corpus(tags_pred[0])\n",
    "print(len(tag_corpus_idx), tags_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZZHZyNSQzHa"
   },
   "source": [
    "### 3. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCGUcr1cZnk5"
   },
   "outputs": [],
   "source": [
    "# 1. Loading LDA model and LDA_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "handle = open('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/LDA_dictionary.pickle', 'rb')\n",
    "dictionary = pickle.load(handle)\n",
    "\n",
    "ldamodel_title_body_tag = LdaModel.load('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/ldamodel_title_body_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyzAF68KgR7L"
   },
   "outputs": [],
   "source": [
    "# 2. defining a final topic prediction function\n",
    "def final_topic_prediction(corpus):\n",
    "  clean_corpus = text_preprocessor(corpus, stop_word = True, remove_digits = True)\n",
    "  tokens_corpus = [i.split(' ') for i in clean_corpus]\n",
    "  BOW_corpus = [dictionary.doc2bow(i) for i in tokens_corpus]\n",
    "\n",
    "  topics_pred = []\n",
    "  for BOW_query in BOW_corpus:\n",
    "    topic_proba_tuple = ldamodel_title_body_tag.get_document_topics(BOW_query, minimum_probability = 0.20)\n",
    "    topics_pred.append([k for k,v in topic_proba_tuple])\n",
    "  return topics_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUs3rxRmbyDu"
   },
   "outputs": [],
   "source": [
    "# 3. Creating a dictionary of {topic_id : datapoint_idx}\n",
    "idx = []\n",
    "topics = list(set(np.concatenate([tbs_df['topic_pred1'].unique(), tbs_df['topic_pred2'].unique(), tbs_df['topic_pred3'].unique(), tbs_df['topic_pred4'].unique()], axis = 0)))\n",
    "topics.remove(1000)\n",
    "for topic in topics:\n",
    "    idx.append(list(tbs_df[(tbs_df['topic_pred1'] == topic) | (tbs_df['topic_pred2'] == topic) | (tbs_df['topic_pred3'] == topic) | (tbs_df['topic_pred4'] == topic)].index))\n",
    "topic_idx_dict = dict(zip(topics, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_XgFmUhfFaK"
   },
   "outputs": [],
   "source": [
    "# 4. final indices of tag_corpus\n",
    "def topic_corpus(topics_pred):\n",
    "  \"\"\"This function takes predicted topics and returns indices of corrospoinding questions from dataset\"\"\"\n",
    "  topic_corpus_idx = []\n",
    "  for topic in topics_pred:\n",
    "    topic_corpus_idx += topic_idx_dict[topic]\n",
    "  return list(set(topic_corpus_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "TFHTAnURl5DT",
    "outputId": "b28a1030-de32-4af3-f02b-0d78dca294ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4778 [[38, 81]]\n",
      "CPU times: user 2.21 ms, sys: 18 µs, total: 2.22 ms\n",
      "Wall time: 2.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FINAL TESTING\n",
    "topics_pred = final_topic_prediction([tbs_df['Title'].values[129792]])\n",
    "topics_corpus_idx = topic_corpus(topics_pred[0])\n",
    "print(len(topics_corpus_idx), topics_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRfBu5grmzrw"
   },
   "source": [
    "### 4. BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "id": "_vo0lPnkx76k",
    "outputId": "df654ad3-44f3-401a-cf00-b04015088067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank-bm25) (1.18.5)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "oaqfk5ucyzd_",
    "outputId": "2ac7be15-e5f3-41e3-d35b-90b038b3d92b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182039"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. preparing dataset for BM25 : truncated \"title + body\"\n",
    "# title_body preprocessing\n",
    "corpus =  tbs_df['combined_text'].values\n",
    "title_body = text_preprocessor(corpus, remove_digits= True, stop_word=True)\n",
    "\n",
    "# truncating title_body on 40 words\n",
    "title_body = [' '.join(i.split(' ')[:40]) for i in title_body]\n",
    "\n",
    "len(title_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PPZHa-v6UN4"
   },
   "outputs": [],
   "source": [
    "# 2. Training BM25 model\n",
    "train_tokens = [i.split(' ') for i in title_body]\n",
    "bm25 = BM25Okapi(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64xfTAPA6grU"
   },
   "outputs": [],
   "source": [
    "# 3. Defining a final function\n",
    "def BM25_corpus(query, train_data, n_results):\n",
    "  # finding results indices\n",
    "  query = text_preprocessor([query], remove_digits= True, stop_word=True)[0]\n",
    "  tokenized_query = query.split(\" \")\n",
    "  idx = range(len(train_data))\n",
    "  BM25_corpus_idx = bm25.get_top_n(tokenized_query, idx, n = n_results)\n",
    "\n",
    "  # getting scores associated with each result\n",
    "  doc_scores = bm25.get_scores(tokenized_query)\n",
    "  BM25_scores = np.sort(doc_scores)[::-1][:n_results]\n",
    "\n",
    "  return BM25_corpus_idx, BM25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UGd0oNPk-Hzv",
    "outputId": "9d42de65-489d-41f5-c332-b51018423fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37109, 129792, 96480, 151612, 55051, 32678, 109072, 11557, 11499, 120617, 104985, 94943, 71311, 23136, 132652, 133116, 87293, 151886, 86985, 126892, 165859, 92932, 25974, 65588, 98599, 178384, 108891, 158708, 6261, 60018, 172472, 163430, 62860, 8748, 41627, 125235, 131273, 181968, 86475, 95657, 10030, 125371, 58555, 86480, 70097, 86753, 151008, 35655, 47587, 174849, 1577, 150926, 62428, 78067, 141734, 123314, 30327, 52858, 95652, 115305, 78564, 140858, 17938, 62582, 18813, 68310, 71043, 89320, 88394, 49729, 181299, 3663, 295, 97846, 130609, 77222, 112365, 58508, 104081, 58557, 84749, 88322, 13084, 49690, 89552, 75008, 11717, 154358, 19535, 45270, 32553, 8287, 120912, 159719, 110735, 68339, 129500, 162342, 125298, 160735] [21.78167401 20.77296241 20.49769274 18.20527936 17.99528755 17.84799521\n",
      " 17.48663342 16.79578485 16.12831391 15.84872905 15.79784663 15.69476294\n",
      " 15.60062952 15.55997442 15.24151023 15.20392383 15.20392383 14.84913141\n",
      " 14.79660187 14.79660187 14.79660187 14.57498002 14.46075685 14.42216656\n",
      " 14.4139983  14.39860648 14.36066789 14.34772406 13.96396219 13.96396219\n",
      " 13.94040211 13.78826666 13.69346139 13.68291708 13.58435152 13.57267573\n",
      " 13.55396894 13.49693847 13.38783322 13.36589319 13.20934501 13.12485745\n",
      " 13.12485745 13.04546549 13.0321616  12.98659966 12.823381   12.72815176\n",
      " 12.72815176 12.72815176 12.72815176 12.60906886 12.59356102 12.49002325\n",
      " 12.42490515 12.42298358 12.34373616 12.32191365 12.25978275 12.24720213\n",
      " 12.20761342 12.15202279 12.03101763 12.03101763 11.99963822 11.99963822\n",
      " 11.9588241  11.90123227 11.89726829 11.89529919 11.83672316 11.83449093\n",
      " 11.8167055  11.80180199 11.79967027 11.78058038 11.74880736 11.56678382\n",
      " 11.50586006 11.45711249 11.45711249 11.44897911 11.44001957 11.43709483\n",
      " 11.43709483 11.43709483 11.43709483 11.43709483 11.43709483 11.41540234\n",
      " 11.41330177 11.39681851 11.39681851 11.39681851 11.33757437 11.25372633\n",
      " 11.23017884 11.15930135 11.15602902 11.12229361] ['Detecting outliers in circular data?'\n",
      " 'Which features to use while detecting outliers in data'\n",
      " 'Which ways should be performed detecting outliers before k-means clustering?'\n",
      " 'When does it make sense to detect multivariate outliers instead of univariate ones?'\n",
      " 'Given a data table with outlier data points, how to determine whether the data table include global outliers or local outliers?'\n",
      " 'Detecting outliers in non parametric data'\n",
      " 'Detecting outliers in contextual time-series data'\n",
      " 'Detecting outliers using standard deviations'\n",
      " 'Is there a simple way of detecting outliers?'\n",
      " 'Outlier detection in high dimensional data'\n",
      " 'Gaussian Mixture for detecting outliers'\n",
      " 'Detecting outliers in non-linear data'\n",
      " \"Cook's distance in detecting outliers\"\n",
      " 'Detecting outliers in a time-series'\n",
      " 'Given data that is labeled as outliers, how can I classify data as outliers?'\n",
      " 'Detecting outliers in binary data using Mahalanobis distance'\n",
      " 'Detecting outliers with angle-based outlier degree'\n",
      " 'How to get top features that contribute to anomalies in Isolation forest'\n",
      " 'Detecting outliers in percentages'\n",
      " 'Unsupervised Outliers detection on time series'\n",
      " 'Detecting the outliers from scatter plot'\n",
      " 'Analysis of railway data - Detecting outliers'\n",
      " 'Detecting outliers using correlogram' 'How to detect outliers?'\n",
      " 'Classification affected by a lot of outliers in features? How do you deal with outliers?'\n",
      " 'Which scaler for sparse dataset with outliers'\n",
      " 'Detecting level of the outlier in the categorical data'\n",
      " 'Detecting outliers in time series with fitting curve'\n",
      " 'Outliers spotting in time series analysis, should I pre-process data or not?'\n",
      " 'Detecting outliers along the distribution in a scatter plot'\n",
      " 'Detecting multivariate outliers in multiple regression'\n",
      " 'What is the method should I use to calculate similarity in a data set with outliers that must be included?'\n",
      " 'how to find outliers from high-dimensional data set?'\n",
      " 'C++ library to play with statistics (detecting outliers in time series)'\n",
      " 'Multivariate outlier detection for PLS model'\n",
      " 'Univariate and multivariate outlier detection'\n",
      " 'Cooks distance and categorical features'\n",
      " 'How can I use statistics to find variables that cause outliers in data?'\n",
      " 'How can we detect the existence of outliers using mean and median?'\n",
      " 'Bonferroni adjustment of studentized residuals for outlier detection'\n",
      " 'Filtering techniques and noise'\n",
      " 'Detecting outlier sequences in clusters'\n",
      " 'Detecting anomalies / outliers in XML structure'\n",
      " 'Can we detect the existence of outliers using mean and median?'\n",
      " 'Outlier removal for univariate and multivariate analysis'\n",
      " 'Can we detect the existence of outliers using Min and Max?'\n",
      " 'Outliers in Regression & Scaling'\n",
      " 'Detecting Outliers in Time Series (LS/AO/TC) using tsoutliers package in R. How to represent outliers in equation format?'\n",
      " 'Basic methods for detecting outliers'\n",
      " 'Detecting the presence outliers, while ignoring pure noise'\n",
      " 'Rigorous definition of an outlier?'\n",
      " 'Workflow in data preparation with Box-Cox transformation'\n",
      " 'How to check the number of outliers in my data?'\n",
      " 'statistics or robust statistics for identifying multivariate outliers'\n",
      " 'Detecting timestamp and feature where anomaly occurs with autoencoder'\n",
      " 'Multivariate outlier detection with isolation forest..How to detect most effective features?'\n",
      " 'Association rules for continuous features'\n",
      " 'Features for Object Detection'\n",
      " 'Normality of the input data and Mahalanobis Taguchi method'\n",
      " 'How to overcome outliers on time series data?'\n",
      " \"Is it possible for 'unsupervised learning' model to recognize features on unlabelled images?\"\n",
      " 'Should I remove outliers if accuracy and Cross-Validation Score drop after removing them?'\n",
      " 'Detecting outliers in count data'\n",
      " 'Detecting outliers in non-normal distribution data'\n",
      " 'Re-check boxplot after outlier removal'\n",
      " 'Outlier detection in multivariate data'\n",
      " 'Standardization with mean/std or median/IQR?'\n",
      " 'Percentage of Outliers In Artificial Data'\n",
      " 'How does ANN deal with outliers?'\n",
      " 'Multivariate Linear Regression with continuous and discrete explanatory variable'\n",
      " 'How to decide which technique to use to treat outliers?'\n",
      " 'If mean is so sensitive, why use it in the first place?'\n",
      " 'On univariate outlier tests (or: Dixon Q versus Grubbs)'\n",
      " 'Use a categorical variable instead of an indicator variable to denote outliers in a regression?'\n",
      " 'How do I remove outliers from my data? Should I use RobustScaler? I am aware I can use DecisionTree but I want to use XGBoost'\n",
      " 'Outlier Detection'\n",
      " 'What the efficient way to normalize data according to different rule?'\n",
      " 'In this classification dataset preprocessing, should outliers be removed before/after reducing dimensionality?'\n",
      " 'Using Standardized Data or Normal Data With Outliers Excluded'\n",
      " 'how we can prove we have outliers in the results?'\n",
      " 'Is it scientifically acceptable to use Prediction Bands to detect/exclude outliers?'\n",
      " 'How to perform data quality check on large number of features using Spark?'\n",
      " 'Outliers for boxplot' 'Correlation analysis while detecting outliers'\n",
      " 'Detecting which independent classification variables result in outliers'\n",
      " 'Comparison of Stahel-Donoho and Minimum Covariance Determinant estimation'\n",
      " 'Detecting initial trend or outliers'\n",
      " '\"Forward search\" methods for outlier detection etc. in regression'\n",
      " 'How accurate is IQR for detecting outliers'\n",
      " 'Removing outliers from data - maximum number of outliers that you can remove?'\n",
      " 'Outliers and Linearity for EFA'\n",
      " 'Improving SVM performance on data with missing features and outliers?'\n",
      " 'DBSCAN input format?'\n",
      " 'Coffee ratings: How to do causal inference with high kurtosis/outliers?'\n",
      " 'Detecting shapes using CNNs in respect to other shapes in the image'\n",
      " 'Can I use the Cook´s Distance to find outliers in a GAM?'\n",
      " 'Kmeans - Does removing outliers on a dimension affect other dimensions clustering prediction?'\n",
      " 'have new outliers after capping' 'Outlier detection for skewed data'\n",
      " 'Gaussian Mixture Model performance with data outliers']\n",
      "CPU times: user 607 ms, sys: 3.07 ms, total: 610 ms\n",
      "Wall time: 611 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Final testing\n",
    "query = tbs_df['Title'][129792]\n",
    "BM25_corpus_idx, BM25_scores = BM25_corpus(query, train_data = train_tokens, n_results = 100)\n",
    "print(BM25_corpus_idx, BM25_scores, tbs_df.Title.values[BM25_corpus_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "7HjLWdJQUiyh",
    "outputId": "5183260f-2bd9-41cd-fb03-e6edde2c6919"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4839"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tag_corpus_idx + topics_corpus_idx + BM25_corpus_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKXE7hNHHoxd"
   },
   "source": [
    "### 5. Combining all corpus indices : 'tag_corpus' + 'topic_corpus' + 'BM25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RorLcksxH5wF"
   },
   "outputs": [],
   "source": [
    "def all_results_idx(query):\n",
    "  # 1. tag_predictor\n",
    "  tags_pred = final_tag_prediction([query])\n",
    "  tag_corpus_idx = tag_corpus(tags_pred[0])\n",
    "\n",
    "  # 2. LDA - topic prediction\n",
    "  topics_pred = final_topic_prediction([query + ' ' + ' '.join(tags_pred[0])]) # adding tags to query\n",
    "  topics_corpus_idx = topic_corpus(topics_pred[0])\n",
    "\n",
    "  # 3. BM25 results\n",
    "  BM25_corpus_idx, BM25_scores = BM25_corpus(query, train_tokens, n_results = 100)\n",
    "\n",
    "  all_idx = list(set(tag_corpus_idx + topics_corpus_idx + BM25_corpus_idx))\n",
    "  return all_idx, dict(zip(BM25_corpus_idx, BM25_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "f5ZMWUaOKrj6",
    "outputId": "c4d7054c-cd10-4b4b-e8d8-fc36d42e3f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4839 Which features to use while detecting outliers in data\n",
      "CPU times: user 642 ms, sys: 2 ms, total: 644 ms\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing\n",
    "query = tbs_df['Title'][129792]\n",
    "all_idx, BM25_dict = all_results_idx(query)\n",
    "\n",
    "print(len(all_idx), query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUyaQGd4Lf-x"
   },
   "source": [
    "### 6.Sentence Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ih0wK2nBiXo"
   },
   "source": [
    "### 6.1 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "LDOcN4W6BoPK",
    "outputId": "efb203a5-1e6c-4b2f-a2f5-dfadda9cd259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 1. Loading BERT vector representation of all questions in the dataset\n",
    "bert_embeddings = np.load('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/bert_train_out.npy')\n",
    "\n",
    "# 2. Laoding pretrained BERT model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "!pip install transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "4KSPa6nfMcb1",
    "outputId": "4fc13d8e-ceb8-4877-8d0d-e6932d594759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bert_input_ids (InputLayer)     [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_attn_mask (InputLayer)     [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_token_typ (InputLayer)     [(None, 44)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 44, 768), (N 109482240   bert_input_ids[0][0]             \n",
      "                                                                 bert_attn_mask[0][0]             \n",
      "                                                                 bert_token_typ[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_bert_model[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. BERT model\n",
    "bert_input_ids = Input(name = 'bert_input_ids', shape = (44,), dtype = 'int64')\n",
    "bert_attn_mask = Input(name = 'bert_attn_mask', shape = (44,), dtype = 'int64')\n",
    "bert_token_typ = Input(name = 'bert_token_typ', shape = (44,), dtype = 'int64')\n",
    "\n",
    "bert_output = bert_model([bert_input_ids, bert_attn_mask, bert_token_typ])\n",
    "bert_output = bert_output[0][:,0,:]\n",
    "# bert_output = bert_output[1][:]\n",
    "\n",
    "BERT = Model(inputs = [bert_input_ids, bert_attn_mask, bert_token_typ], outputs = bert_output)\n",
    "BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgsidikXabf_"
   },
   "outputs": [],
   "source": [
    "# 4. input text preprocessor\n",
    "def text_preprocessor(corpus, stop_word = False, remove_digits = False):\n",
    "  clean_corpus = []\n",
    "  for doc in corpus:\n",
    "    # 1. remove html tags, html urls, replace html comparison operators\n",
    "    clean_str = re.sub('<.*?>', '', doc)\n",
    "    clean_str = clean_str.replace('&lt;', '<')\\\n",
    "                .replace('&gt;', '>')\\\n",
    "                .replace('&le;', '<=' )\\\n",
    "                .replace('&ge;', '>=')\n",
    "\n",
    "    # 2. remove latex i,e., mostly formulas since it's mathematics based dataset\n",
    "    clean_str = re.sub('\\$.*?\\$', '', clean_str)\n",
    "\n",
    "    # 3. all lowercase \n",
    "    clean_str = clean_str.lower()\n",
    "\n",
    "    # 4. decontractions\n",
    "    clean_str = clean_str.replace(\"won't\", \"will not\").replace(\"can\\'t\", \"can not\").replace(\"n\\'t\", \" not\").replace(\"\\'re\", \" are\").\\\n",
    "                                                  replace(\"\\'s\", \" is\").replace(\"\\'d\", \" would\").replace(\"\\'ll\", \" will\").\\\n",
    "                                                  replace(\"\\'t\", \" not\").replace(\"\\'ve\", \" have\").replace(\"\\'m\", \" am\")\n",
    "\n",
    "    # # 5. remove all special-characters other than alpha-numericals\n",
    "    clean_str = re.sub('\\W', ' ', clean_str)\n",
    "    if remove_digits == True:\n",
    "      clean_str = re.sub('\\d', ' ', clean_str)\n",
    "\n",
    "    # 6. Stop_word removal\n",
    "    if stop_word == True:\n",
    "      clean_str = stopwrd_removal(clean_str)\n",
    "\n",
    "    # 7. remove all white-space i.e., \\n, \\t, and extra_spaces\n",
    "    clean_str = re.sub('  +', ' ', clean_str)\n",
    "    clean_str = clean_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "\n",
    "    clean_corpus.append(clean_str)\n",
    "\n",
    "  return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmkDqltPB9h8"
   },
   "outputs": [],
   "source": [
    "# 5. A function to create vector representation of query\n",
    "def BERT_sentence_vec(query):\n",
    "  clean_query = text_preprocessor([query],  stop_word = False, remove_digits = False)\n",
    "  tokens = bert_tokenizer.batch_encode_plus(clean_query, truncation = True, max_length =  44, pad_to_max_length = True)\n",
    "\n",
    "  input_ids = np.array(tokens['input_ids'])\n",
    "  attn_mask = np.array(tokens['attention_mask'])\n",
    "  token_typ_ids = np.array(tokens['token_type_ids'])\n",
    "  bert_out = BERT.predict([input_ids, attn_mask, token_typ_ids])\n",
    "  return bert_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "OCihwrP3DZkZ",
    "outputId": "56603eb8-4f1b-4609-f610-a83889032b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.094334483146667e-07,\n",
       " 'How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?')"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross checking the current results with previous results\n",
    "query = tbs_df['Title'][50]\n",
    "sum(bert_embeddings[50] - BERT_sentence_vec(query)[0]), query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "cQYk6nioCPSj",
    "outputId": "23f49ca7-5faf-4f0b-c82b-4904995f04f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "CPU times: user 70.4 ms, sys: 3.03 ms, total: 73.5 ms\n",
      "Wall time: 61.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 6. Final testing of BERT model with query point\n",
    "query = \"How does deepmind's Atari game AI work?\"\n",
    "bert_out = BERT_sentence_vec(query)\n",
    "print(bert_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMFcWoq6C3ck"
   },
   "source": [
    "### 6.2. USE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pxi0YynaPLqp"
   },
   "outputs": [],
   "source": [
    "# 1. Loading USE vector representation of all questions in the dataset\n",
    "use_embeddings = np.load('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/use_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "luMBgawQLp_p",
    "outputId": "d29a500e-4801-436a-8361-0424232178a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n"
     ]
    }
   ],
   "source": [
    "# 2. Laoding pretrained USE model\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gi7b8HUTstnx"
   },
   "outputs": [],
   "source": [
    "# 3. A function to create vector representation of query\n",
    "def USE_sentence_vec(query):\n",
    "  clean_query = text_preprocessor([query],  stop_word = False, remove_digits = False)\n",
    "  use_out = use_model(clean_query)\n",
    "\n",
    "  return use_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Xpjf0fvPXHsX",
    "outputId": "962e1b2b-372f-457e-ce2e-8404cd4e619c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=7.386261e-07>,\n",
       " 'Why would a BaggingRegressor only use a subset of samples and features during fitting?')"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross checking the current results with previous results\n",
    "query = tbs_df['Title'][100000]\n",
    "sum(use_embeddings[100000] - USE_sentence_vec(query)[0]), query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "ZK96wzSwO2kf",
    "outputId": "95d11e76-10c2-4fc5-9bf8-6bd810566e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "CPU times: user 34.2 ms, sys: 4.3 ms, total: 38.5 ms\n",
      "Wall time: 29.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4. Final testing of BERT model with query point\n",
    "query = \"How does deepmind's Atari game AI work?\"\n",
    "use_out = USE_sentence_vec(query)\n",
    "print(use_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfMhAVN4QY_Q"
   },
   "source": [
    "### 7. Ranking : compute cos-sim based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q--kBSSaM7MV"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def cos_sim(a, b):\n",
    "  cos_sim = np.dot(a, b)/(norm(a)*norm(b))\n",
    "  return cos_sim\n",
    "\n",
    "def inverse_euc_dist(a, b):\n",
    "  euc_dist = norm(a-b)\n",
    "  return 1/euc_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znBwtRtfprNr"
   },
   "source": [
    "**Final search results using : BERT embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WheDYk2vHoP0"
   },
   "outputs": [],
   "source": [
    "def BERT_results(query, n = 10):\n",
    "  all_idx, BM25_dict = all_results_idx(query)\n",
    "  BERT_corpus = bert_embeddings[all_idx]\n",
    "\n",
    "  query_vector = BERT_sentence_vec(query)[0]\n",
    "  cos_sim_lst = [cos_sim(query_vector, b) for b in BERT_corpus]\n",
    "\n",
    "  d = dict(zip(all_idx, cos_sim_lst))\n",
    "  cos_sim_idx = list(dict(sorted(d.items(), key=lambda x: x[1], reverse=True)).keys())\n",
    "\n",
    "  return tbs_df.Title.values[cos_sim_idx][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "QQQIWOr9IBXO",
    "outputId": "7657b7fe-1442-4c65-9dad-eaab092dd038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Keras transpose' 'Reflective padding as pure keras verision'\n",
      " 'Problems with accuracy.score sklearn'\n",
      " 'SKLearn DT regressor - good enough score?'\n",
      " 'Keras bug NasNetlarge no top' 'What to do after GridSearchCV()?'\n",
      " 'Keras prediction' 'how to change keras backend in windows?'\n",
      " 'Tips for optimizing my Keras Model'\n",
      " 'Error in training a merged model in Keras']\n",
      "CPU times: user 504 ms, sys: 5.74 ms, total: 510 ms\n",
      "Wall time: 495 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# some sample queries to try: \n",
    "# tensorflow vs pytorch # difference between tensorflow and pytorch # keras accuracy stuck # change keras backend\n",
    "# what is the best deep learning library for scala # install nltk # optimizing overfitted models\n",
    "results = BERT_results(query = \"change keras backend\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrI6-03MqP-x"
   },
   "source": [
    "**Final search results using : USE embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLrkef7NIXtM"
   },
   "outputs": [],
   "source": [
    "def USE_results(query, n = 10):\n",
    "  all_idx, BM25_dict = all_results_idx(query)\n",
    "  BERT_corpus = use_embeddings[all_idx]\n",
    "\n",
    "  query_vector = USE_sentence_vec(query)[0]\n",
    "  cos_sim_lst = [cos_sim(query_vector, b) for b in BERT_corpus]\n",
    "\n",
    "  d = dict(zip(all_idx, cos_sim_lst))\n",
    "  cos_sim_idx = list(dict(sorted(d.items(), key=lambda x: x[1], reverse=True)).keys())\n",
    "\n",
    "  return tbs_df.Title.values[cos_sim_idx][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "colab_type": "code",
    "id": "RzCHW4H7I6zE",
    "outputId": "e17597cf-9f19-4a9f-ddef-9ba3e3e3ab0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Switching Keras backend Tensorflow to GPU'\n",
      " 'how to change keras backend in windows?'\n",
      " 'Converting a Keras model to PyTorch' 'Changing padding values in Keras'\n",
      " 'Keras custom layer using tensorflow function' 'Keras Import Error'\n",
      " 'How do i pass data into keras?' 'Keras update N batches'\n",
      " 'Feed data into Keras LSTM layer' 'Feed data into Keras LSTM layer']\n",
      "CPU times: user 505 ms, sys: 4.79 ms, total: 510 ms\n",
      "Wall time: 501 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = USE_results(query = \"change keras backend\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crucial points to note:\n",
    "**1. The purpose of this case study is to build simple search engine with LOW latency.**\n",
    "\n",
    "**2. By using 'right data structures' in our mechanism, we make it happen to get results under 800-900 milliseconds on a normal 8 gb machine.**\n",
    "\n",
    "# Observations : \n",
    "    1. pretrained BERT model is not trained to capture semantic relationships at first place. Its trained on two tasks : NSP (Next sentence prediction) and MLM (Masked language model).\n",
    "    2. The corpus on which BERT model is trained is general wikipidia data, But our stackoverflow corpus has all the mathematical, computer science and machine learning related technical terms.\n",
    "    3. first things first Universal sentence embedding model is trained to capture semantic relationships with contextual meaning.\n",
    "    \n",
    "# Conclusion :\n",
    "    1. Hence BERT model fails to give good results as compare to USE model.\n",
    "    2. We need to fine tune bert model on our technical corpus to get good results with bert.\n",
    "    \n",
    "- In our case USE embeddings outperformed. A searching mechanism with USE vectors gives great reults with 'semantic relationship' between query and resulting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Comparison with stackoverflow.com results\n",
    "\n",
    "find all results here : https://imgur.com/a/9XRVEOd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1  =  'How to reverse a linked list in python'\n",
    "**top 5 results :**\n",
    "\n",
    " <img src='https://i.imgur.com/rbCbWua.png' width=\"800\">\n",
    "\n",
    " **next 5 results :**\n",
    "\n",
    " <img src='https://i.imgur.com/6B4bNU1.png' width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "3eHMhz2aHh6M",
    "outputId": "adb895bb-0bce-4af5-b5a4-5a7a49410528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How to anonymize (de-identify) data in Python?'\n",
      " 'How to run AgglomerativeClustering on a big data in python?'\n",
      " 'How to deal with non-numeric missing values with python'\n",
      " 'How to convert a SQLContext Dataframe to RDD of vectors in Python?'\n",
      " 'How to stratify a dataset to keep groups of data together in Python?'\n",
      " 'How to work with large amount of data overcoming RAM issues in python'\n",
      " 'How to create indexes from \"open\" variables'\n",
      " 'How to convert categorical data to numerical data in Pyspark'\n",
      " 'How to replace short words into full words from tweets using python'\n",
      " 'How can I detect partially obscured objects using Python?']\n",
      "CPU times: user 680 ms, sys: 1.61 ms, total: 682 ms\n",
      "Wall time: 664 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = BERT_results(query = \"How to reverse a linked list in python\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "INtpIrUMJ6mf",
    "outputId": "cc851de5-b037-4910-c750-a9938fef20ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['print the nodes of an immutable single-linked list in reverse order'\n",
      " 'Using singly linked list instead of a doubly linked list?'\n",
      " 'How to find middle element of doubly linked list using head and tail?'\n",
      " 'How to find middle element of linked list in one pass?'\n",
      " 'Correct way to implement linked list'\n",
      " 'Sort doubly linked list efficiently'\n",
      " 'Finding the nth to last node in a linked list'\n",
      " 'concatenating the content of list in python'\n",
      " 'A question about linked list' 'Linked lists with auxiliary data']\n",
      "CPU times: user 671 ms, sys: 4.44 ms, total: 676 ms\n",
      "Wall time: 676 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = USE_results(query = \"How to reverse a linked list in python\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2  = 'valueerror'\n",
    "\n",
    "**top 5 results :**\n",
    "\n",
    "<img src='https://i.imgur.com/8SlM6lH.png' width=\"800\">\n",
    "\n",
    "**next 5 results :**\n",
    "\n",
    " <img src='https://i.imgur.com/AYcDUsI.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "NtmSOAFmE5P5",
    "outputId": "b6bcc897-a6d1-45d7-9a8c-75a15468acb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Keras save model FailedPreconditionError'\n",
      " 'Keras mnist.load_data() unshuffled?' 'Groupby product, return tuple'\n",
      " \"AttributeError: 'numpy.ndarray' object has no attribute 'predict'\"\n",
      " 'LogisticRegression - binary classification, \"custom threshold\"'\n",
      " 'Keras input dimension bug?' 'XGBClassifier default scoring metric'\n",
      " 'OneVsRestClassifier and predict_proba' 'Keras Custom Loss Function'\n",
      " 'liblinear one vs rest learn parameters']\n",
      "CPU times: user 294 ms, sys: 6.08 ms, total: 300 ms\n",
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = BERT_results(query = \"valueerror\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "OHNJayBDPOZa",
    "outputId": "27d5b663-ff64-4577-e2a3-ebb7ef33179f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ValueError from statsmodels ExponentialSmoothing'\n",
      " 'Getting a ValueError from train_test_split'\n",
      " 'XGBClassifier error! ValueError: feature_names mismatch:'\n",
      " 'train_test_split ValueError: Input contains NaN'\n",
      " 'ValueError: not enough values to unpack (expected 4, got 2)'\n",
      " \"ValueError: ('Error when checking model input: expected no data, but got:', array)\"\n",
      " 'ValueError while Comparing with Multilevel Index'\n",
      " 'ValueError while using linear regression'\n",
      " \"TypeError: unhashable type: 'numpy.ndarray'\"\n",
      " \"TypeError: unhashable type: 'numpy.ndarray''\"]\n",
      "CPU times: user 272 ms, sys: 6.96 ms, total: 279 ms\n",
      "Wall time: 273 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = USE_results(query = \"valueerror\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3  = 'matplotlib'\n",
    "\n",
    "**top 5 results :**\n",
    "\n",
    "<img src='https://i.imgur.com/EryAZE7.png' width=\"800\">\n",
    "\n",
    "**next 5 results :**\n",
    "\n",
    " <img src='https://i.imgur.com/aBc6X0O.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "mkVkUGt2QImT",
    "outputId": "1d83020e-19ea-4625-eb56-4a3ea9574605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conceptual clustering with sklearn?'\n",
      " 'Sklearn StratifiedKFold code explanation'\n",
      " 'Should I delete the intercept' 'Feature Importance from a GridSearchCV'\n",
      " 'Does $\\\\mathbb{P}(XY=a)=\\\\mathbb{P}(X=a)\\\\mathbb{P}(Y=a)$ for $X,Y$ id?'\n",
      " 'Decision Trees split in scikit' 'mvBacon from R to Python'\n",
      " 'MinMaxScaler broadcast shapes'\n",
      " \"Scikit-learn's SGDClassifier code question\"\n",
      " 'Using TF-IDF with other features in SKLearn']\n",
      "CPU times: user 282 ms, sys: 5.84 ms, total: 288 ms\n",
      "Wall time: 273 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = BERT_results(query = \"matplotlib\", n = 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "K-rTwo-RJtgZ",
    "outputId": "4e18cf01-b4cb-447b-d6bc-1abe9c39a5ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using Matplotlib' 'matplotlib graph to plot values and variance'\n",
      " 'How to plot a contour map in python using matplotlib?'\n",
      " 'How do I fix mis-rendered matplotlib?'\n",
      " 'How to annotate labels in a 3D matplotlib scatter plot?'\n",
      " 'Error plotting with datetime and value using matplotlib in python'\n",
      " 'A better way of visualizing extreme oscillation curve in matplotlib?'\n",
      " 'How to plot a 3-axis bar chart with matplotlib (and pandas + jupyter)'\n",
      " 'How to plot 2D or 3D graph using Python?'\n",
      " 'Matplotlib Plot Difference Between Two Unsorted Value Series']\n",
      "CPU times: user 277 ms, sys: 1.71 ms, total: 279 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = USE_results(query = \"matplotlib\", n = 10)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "6.1. Final Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
