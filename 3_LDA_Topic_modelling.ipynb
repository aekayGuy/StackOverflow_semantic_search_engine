{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. LDA - Topic modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mQqp22poqLWxQbTE5bkRhlx1qLCoRL-H",
      "authorship_tag": "ABX9TyMhy57PZDSKekyuMTa5cZt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vispute/StackOverflow_semantic_search_engine/blob/master/3_LDA_Topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHhP3FkV5ps7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk0MdHSj6SIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Acquiring preprocessed_dataset\n",
        "tbs_df = pd.read_csv('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/tbs_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82GL8gprCmOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f2ebb8ea-684e-487d-866c-1fd44efd3da7"
      },
      "source": [
        "# defining a function to remove stop_words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('would')\n",
        "stop_words.update([chr(c) for c in range(97, 123)])\n",
        "# stop_words.remove('no'); stop_words.remove('not'); stop_words.remove('nor')\n",
        "\n",
        "def stopwrd_removal(sent):\n",
        "  lst = []\n",
        "  for wrd in sent.split():\n",
        "    if wrd not in stop_words:\n",
        "      lst.append(wrd)\n",
        "  return \" \".join(lst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx7gMPbx6ejV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocessor(column):\n",
        "  \"\"\"pass any column with Text in it from tbs_df | Note: returns nothing makes inplace changes in tbs_df\"\"\"\n",
        "  # 1. remove html tags, html urls, replace html comparison operators\n",
        "  text = tbs_df[column].values\n",
        "  tbs_df[column] = [re.sub('<.*?>', '', i) for i in text]\n",
        "  tbs_df[column] = tbs_df[column].str.replace('&lt;', '<')\\\n",
        "                                          .str.replace('&gt;', '>')\\\n",
        "                                          .str.replace('&le;', '<=' )\\\n",
        "                                          .str.replace('&ge;', '>=')\n",
        "\n",
        "  # 2. remove latex i,e., mostly formulas since it's mathematics based dataset\n",
        "  tbs_df[column] = [re.sub('\\$.*?\\$', '', i) for i in text]\n",
        "\n",
        "  # 3. all lowercase \n",
        "  tbs_df[column] = tbs_df[column].str.lower()\n",
        "\n",
        "  # 4. decontractions\n",
        "  tbs_df[column] = tbs_df[column].str.replace(\"won't\", \"will not\").str.replace(\"can\\'t\", \"can not\").str.replace(\"n\\'t\", \" not\").str.replace(\"\\'re\", \" are\").str.\\\n",
        "                                                replace(\"\\'s\", \" is\").str.replace(\"\\'d\", \" would\").str.replace(\"\\'ll\", \" will\").str.\\\n",
        "                                                replace(\"\\'t\", \" not\").str.replace(\"\\'ve\", \" have\").str.replace(\"\\'m\", \" am\")\n",
        "\n",
        "  # 5. remove all special-characters other than alpha-numericals\n",
        "  tbs_df[column] = [re.sub('\\W', ' ', i) for i in text]\n",
        "  # remove all digits\n",
        "  tbs_df[column] = [re.sub('\\d', ' ', i) for i in text]\n",
        "\n",
        "  # 6. Stop_word removal\n",
        "  tbs_df[column] = [stopwrd_removal(i) for i in text]\n",
        "\n",
        "  # 7. remove all white-space i.e., \\n, \\t, and extra_spaces\n",
        "  tbs_df[column] = [re.sub('  +', ' ', i) for i in text]\n",
        "  tbs_df[column] = tbs_df[column].str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UjPXHWE5WVV",
        "colab_type": "text"
      },
      "source": [
        "**Note: For LDA modelling I am removing all digits and stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaTCGTzP6xeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "44f083db-6dbc-4fb8-ce36-8b06e5549567"
      },
      "source": [
        "# 1. train_test split\n",
        "train_set = 0.80\n",
        "test_set = 1 - train_set\n",
        "text_preprocessor('combined_text')\n",
        "\n",
        "# 2. splitting 'combined_text'\n",
        "title_body_train = tbs_df['combined_text'].values[:int(tbs_df.shape[0]*train_set)]\n",
        "title_body_test = tbs_df['combined_text'].values[-int(tbs_df.shape[0]*test_set):]\n",
        "\n",
        "# 3. tags splitting\n",
        "tags = tbs_df['tag_pred1'].str.cat(tbs_df['tag_pred2'], sep = ' ').str.cat(tbs_df['tag_pred3'], sep = ' ').str.cat(tbs_df['tag_pred4'], sep = ' ').str.cat(tbs_df['tag_pred5'], sep = ' ').tolist()\n",
        "tags_train = np.array(tags[:int(tbs_df.shape[0]*train_set)])\n",
        "tags_test = np.array(tags[-int(tbs_df.shape[0]*test_set):])\n",
        "\n",
        "title_body_train.shape, title_body_test.shape, tags_train.shape, tags_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((145631,), (36407,), (145631,), (36407,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4a2bYtaSd7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. truncating title_body on 60 words\n",
        "title_body_train = [' '.join(i.split(' ')[:60]) for i in title_body_train]\n",
        "title_body_test = [' '.join(i.split(' ')[:60]) for i in title_body_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13HYtnxK7U59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5. cleaning tags i.e., removing special characters\n",
        "clean_train_tags = []\n",
        "for i in tags_train:\n",
        "  clean_train_tags.append(re.sub('[<>-]', \"\", i).strip())\n",
        "\n",
        "clean_test_tags = []\n",
        "for i in tags_test:\n",
        "  clean_test_tags.append(re.sub('[<>-]', \"\", i).strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTZB1XzUA1O7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6. Joining 'combined_text' + 'Tags'\n",
        "final_train = [i + ' ' + j for i, j in zip(title_body_train, clean_train_tags)]\n",
        "final_test = [i + ' ' + j for i, j in zip(title_body_test, clean_test_tags)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oleRKp9dzpAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "82f630dc-635f-4f26-81b6-588d0c5d5205"
      },
      "source": [
        "final_train[2480:2490]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['symbolic computer algebra statistics functionality exist cas specifically geared toward statistics symbolic algebra systems like mathematica maple often used calculus logic physics problems rarely used statistics statistical constructs could added symbolic algebra system improve use field specific code samples many people like able please think following three users research statistician non statistics researcher using statistics another field biology statistics student computational_statistics',\n",
              " 'use rejection sampling generate draws unit exponential working practice test problems one says design rejection sampling algorithm produce draws unit exponential using draws gamma understand possible impression envelope function needs scalable manner constant see way gamma going little mass around exponential function mass around kind transformation need gamma function allow function envelope using tried flipping make inverse gamma adequately capture random_generation',\n",
              " ' mathematical_statistics',\n",
              " 'bootstrapping data envelopment analysis efficiency score using want perform bootstrapping calculation efficiency score data envelopment analysis dea using examples data results type analysis enable check results online resources might assist task bootstrap',\n",
              " 'panel data fixed effects model auto correlation introduce bias given panel countries time fixed effects estimator makes sense control country specific effects intuition tells dependent variable correlated lags independent variables bias introduced estimator however difficulty rigorously understanding case additionally easy way tell bias towards away zero panel_data fixed_effects_model',\n",
              " 'estimating variance unbiased estimators divide yet maximum likelihood estimates divide totally confused one hand read kinds explanations divide get unbiased estimator unknown population variance degrees freedom defined sample size etc see hand comes variance estimation supposed normal distribution seem true anymore said maximum likelihood estimator variance includes division see anyone please enlighten true mean normality models boil least due clt unbiased_estimator maximum_likelihood',\n",
              " 'problematic one predictor set accounts almost prediction running logistic regression customer event data multiple predictors however one variable extremely important alone predicting customers event main predictor included model predictors add little prediction main predictor main predictor post event variable variable full business support model given still okay retain main predictor variable model suggest anything wrong model predictor',\n",
              " 'making heatmap precomputed distance matrix data matrix made heatmap based upon regular data matrix package use pheatmap regular clustering samples performed distfun function within package want attach precomputed distance matrix generated unifrac previously generated matrix heatmap possible matrix data_visualization',\n",
              " 'advice missing value imputation working insurance data customer field named customer_no_dependent customer number dependent coming significant variable variable almost missing values imputation thought determine proxy indicators number dependents tried age thinking person aged could dependents correlated premium amount well think person dependents could less disposable income low premium paying could meaning dependents understand demographic variable fully taken logic somebody goes missing_data data_imputation',\n",
              " 'analyse repeated measure anova three conditions presented randomised order context question concerns typical design area researcher takes group subjects say applies three different conditions measure change response variable vertical jump height performed drinking glucose drink coloured plain water fruit juice say every subject every treatment random order enough time effects wash analysis kuehl kuehl design experiments statistical principles research design repeated_measures anova']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbK48ML3jzKD",
        "colab_type": "text"
      },
      "source": [
        "# 8.1. LDA Model : Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8kSYW6K_f0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. creating BOW Matrix - gensim returns tuple of (token_id in dict, frequency)\n",
        "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "train_tokens = [i.split(' ') for i in final_train]\n",
        "dictionary = corpora.Dictionary(train_tokens)\n",
        "train_BOW = [dictionary.doc2bow(i) for i in train_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orqxgI2pdEXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/LDA_dictionary.pickle', 'wb') as handle:\n",
        "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm6lxsDAIgNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Preparing test set\n",
        "test_tokens = [i.split(' ') for i in final_test]\n",
        "test_BOW = [dictionary.doc2bow(i) for i in test_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5AQ5EWuE8_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Training LDA model on BOW train matrix\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "ldamodel_title_body_tag = LdaModel(train_BOW, num_topics = 250, id2word = dictionary, passes = 10, random_state = 101, update_every = 128)\n",
        "ldamodel_title_body_tag.save('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/ldamodel_title_body_tag')\n",
        "ldamodel_title_body_tag = LdaModel.load('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/ldamodel_title_body_tag')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eES-p5XN511M",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Testing LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtn7JTCovw7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ca604dcc-ff75-4b16-c4b6-20c698a12638"
      },
      "source": [
        "# 4. Testing LDA model\n",
        "x = ldamodel_title_body_tag[test_BOW[:5]]\n",
        "for i in x:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(42, 0.39686412), (113, 0.103119574), (128, 0.14167082), (149, 0.13142478), (191, 0.10757557), (239, 0.103853054)]\n",
            "[(33, 0.071046695), (72, 0.11917574), (117, 0.5029193), (180, 0.19618227), (233, 0.09486958)]\n",
            "[(17, 0.06788025), (167, 0.12746437), (195, 0.1024162), (236, 0.53147554), (244, 0.15495716)]\n",
            "[(1, 0.08113217), (29, 0.04023653), (44, 0.018637098), (71, 0.1350893), (80, 0.20825763), (166, 0.2095908), (214, 0.06706375), (217, 0.08086122), (224, 0.062055305), (226, 0.034199007), (233, 0.04745785)]\n",
            "[(42, 0.19511972), (166, 0.33886144), (229, 0.44585556)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPE7Y0x-EZqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "8f450719-f6d4-47db-a9f8-1adff1047513"
      },
      "source": [
        "x = ldamodel_title_body_tag.get_document_topics(test_BOW[:5], minimum_probability = 0.20)\n",
        "for i in x:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(42, 0.39710984)]\n",
            "[(117, 0.5304948)]\n",
            "[(236, 0.4874936)]\n",
            "[(80, 0.21931224)]\n",
            "[(166, 0.33872607), (229, 0.4466301)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH3yG1W36xvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "6216618f-2435-4c6b-8e4c-88f2e08f49a4"
      },
      "source": [
        "ldamodel_title_body_tag.get_topic_terms(73, topn = 5),  ldamodel_title_body_tag.show_topic(73, topn = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(697, 0.02759998),\n",
              "  (135, 0.025255308),\n",
              "  (298, 0.023426188),\n",
              "  (78, 0.021936793),\n",
              "  (692, 0.01676521)],\n",
              " [('survey', 0.02759998),\n",
              "  ('scale', 0.025255308),\n",
              "  ('likert', 0.023426188),\n",
              "  ('data', 0.021936793),\n",
              "  ('questions', 0.01676521)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZVwMe-Ntgyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bc028a50-e2e2-4f0f-f5f8-31f323b7783b"
      },
      "source": [
        "len(ldamodel_title_body_tag.print_topics(num_topics = -1, num_words = 5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMG8h1EQKj8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "51607ec2-529c-4663-b43e-c404458931c2"
      },
      "source": [
        "x = ldamodel_title_body_tag[test_BOW[16]]\n",
        "[ldamodel_title_body_tag.show_topic(topicid = i[0], topn = 5) for i in x], final_test[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[('na', 0.05231732),\n",
              "   ('data', 0.0281994),\n",
              "   ('curves', 0.014549139),\n",
              "   ('like', 0.009442198),\n",
              "   ('set', 0.0067460616)],\n",
              "  [('problem', 0.05177094),\n",
              "   ('np', 0.030071674),\n",
              "   ('complete', 0.019165054),\n",
              "   ('set', 0.01636283),\n",
              "   ('polynomial', 0.014421305)]],\n",
              " 'checking combinatorics modelling flash memory system requests writes take cycles complete read take system handle requests proportions based observed values timings arbitrary calculation assuming system fully loaded means probability read write read writes read writes read writes happily adds correct combinatorics rusty online calculators allow proportions combinatorics')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DibuJY8JJA-R",
        "colab_type": "text"
      },
      "source": [
        "# 8.2 LDA Model : predicting whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCGUcr1cZnk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Loading trained LDA model and LDA_dictionary\n",
        "import pickle\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "handle = open('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/LDA_dictionary.pickle', 'rb')\n",
        "dictionary = pickle.load(handle)\n",
        "\n",
        "ldamodel_title_body_tag = LdaModel.load('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/ldamodel_title_body_tag')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F42lUb3w7oZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. input text preprocessor\n",
        "def text_preprocessor(corpus, stop_word = False, remove_digits = False):\n",
        "  clean_corpus = []\n",
        "  for doc in corpus:\n",
        "    # 1. remove html tags, html urls, replace html comparison operators\n",
        "    clean_str = re.sub('<.*?>', '', doc)\n",
        "    clean_str = clean_str.replace('&lt;', '<')\\\n",
        "                .replace('&gt;', '>')\\\n",
        "                .replace('&le;', '<=' )\\\n",
        "                .replace('&ge;', '>=')\n",
        "\n",
        "    # 2. remove latex i,e., mostly formulas since it's mathematics based dataset\n",
        "    clean_str = re.sub('\\$.*?\\$', '', clean_str)\n",
        "\n",
        "    # 3. all lowercase \n",
        "    clean_str = clean_str.lower()\n",
        "\n",
        "    # 4. decontractions\n",
        "    clean_str = clean_str.replace(\"won't\", \"will not\").replace(\"can\\'t\", \"can not\").replace(\"n\\'t\", \" not\").replace(\"\\'re\", \" are\").\\\n",
        "                                                  replace(\"\\'s\", \" is\").replace(\"\\'d\", \" would\").replace(\"\\'ll\", \" will\").\\\n",
        "                                                  replace(\"\\'t\", \" not\").replace(\"\\'ve\", \" have\").replace(\"\\'m\", \" am\")\n",
        "\n",
        "    # 5. remove all special-characters other than alpha-numericals\n",
        "    clean_str = re.sub('\\W', ' ', clean_str)\n",
        "    if remove_digits == True:\n",
        "      clean_str = re.sub('\\d', ' ', clean_str)\n",
        "\n",
        "    # 6. Stop_word removal\n",
        "    if stop_word == True:\n",
        "      clean_str = stopwrd_removal(clean_str)\n",
        "\n",
        "    # 7. remove all white-space i.e., \\n, \\t, and extra_spaces\n",
        "    clean_str = re.sub('  +', ' ', clean_str)\n",
        "    clean_str = clean_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
        "\n",
        "    clean_corpus.append(clean_str)\n",
        "\n",
        "  return clean_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyzAF68KgR7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. defining a final topic prediction function\n",
        "def final_topic_prediction(corpus):\n",
        "  clean_corpus = text_preprocessor(corpus, stop_word = True, remove_digits = True)\n",
        "  tokens_corpus = [i.split(' ') for i in clean_corpus]\n",
        "  BOW_corpus = [dictionary.doc2bow(i) for i in tokens_corpus]\n",
        "\n",
        "  topics_pred = []\n",
        "  for BOW_query in BOW_corpus:\n",
        "    topic_proba_tuple = ldamodel_title_body_tag.get_document_topics(BOW_query, minimum_probability = 0.20)\n",
        "    topics_pred.append(list(dict(topic_proba_tuple).keys()))\n",
        "  return topics_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMc0qSxyp3pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. preparing whole dataset : title + predicted_tags\n",
        "title = tbs_df['Title'].values\n",
        "tags = tbs_df['tag_pred1'].str.cat(tbs_df['tag_pred2'], sep = ' ').str.cat(tbs_df['tag_pred3'], sep = ' ').str.cat(tbs_df['tag_pred4'], sep = ' ').str.cat(tbs_df['tag_pred5'], sep = ' ').tolist()\n",
        "clean_tags = []\n",
        "for i in tags:\n",
        "  clean_tags.append(re.sub('[<>-]', \"\", i).strip())\n",
        "corpus = [i + ' ' + j for i, j in zip(title, clean_tags)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VrX0rPXjUDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "14d9b7f1-28b8-492c-b482-2784534291a2"
      },
      "source": [
        "%%time\n",
        "# 5. predicting topics for whole dataset\n",
        "topic_id_lst = final_topic_prediction(corpus)\n",
        "topic_id_lst[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 57s, sys: 522 ms, total: 2min 58s\n",
            "Wall time: 2min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLp9XXuNhEQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d3454d35-121d-46f3-fe6f-4bc108c8b1c2"
      },
      "source": [
        "# 4. creating new colummns with predicted topics\n",
        "print('maximum no.of topics one doc can have:', max([len(i) for i in topic_id_lst]))\n",
        "tbs_df = pd.concat([tbs_df, pd.DataFrame(topic_id_lst, columns = ['topic_pred1', 'topic_pred2', 'topic_pred3', 'topic_pred4'])], axis = 1)\n",
        "# replacing nan values with 1000 to maintain the pandas series dtype = numerics\n",
        "tbs_df = tbs_df.fillna(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum no.of topics one doc can have: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-49RGG5hXn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tbs_df.to_csv('/content/drive/My Drive/AAIC Course/Personal case study - StackOverflow/tbs_df.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORk4R8X1hXjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57ca89ff-6a91-463a-a3cd-33c7d0a181c4"
      },
      "source": [
        "tbs_df.iloc[2480:2490, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>LastActivityDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>ViewCount</th>\n",
              "      <th>AnswerCount</th>\n",
              "      <th>CommentCount</th>\n",
              "      <th>FavoriteCount</th>\n",
              "      <th>Comments</th>\n",
              "      <th>index_left</th>\n",
              "      <th>tag_1</th>\n",
              "      <th>tag_2</th>\n",
              "      <th>tag_3</th>\n",
              "      <th>tag_4</th>\n",
              "      <th>tag_5</th>\n",
              "      <th>index</th>\n",
              "      <th>combined_text</th>\n",
              "      <th>sentiment_comb</th>\n",
              "      <th>subjectivity_comb</th>\n",
              "      <th>sentiment_comments</th>\n",
              "      <th>subjectivity_comments</th>\n",
              "      <th>UNIX_CreationDate</th>\n",
              "      <th>Title_1</th>\n",
              "      <th>tag_pred1</th>\n",
              "      <th>tag_pred2</th>\n",
              "      <th>tag_pred3</th>\n",
              "      <th>tag_pred4</th>\n",
              "      <th>tag_pred5</th>\n",
              "      <th>topic_pred1</th>\n",
              "      <th>topic_pred2</th>\n",
              "      <th>topic_pred3</th>\n",
              "      <th>topic_pred4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>Symbolic computer algebra for statistics</td>\n",
              "      <td>&lt;p&gt;What functionality should exist in a &lt;a hre...</td>\n",
              "      <td>&lt;python&gt;&lt;computational_statistics&gt;&lt;computing&gt;&lt;...</td>\n",
              "      <td>2011-05-04T20:32:27.960</td>\n",
              "      <td>2019-01-19T23:04:45.170</td>\n",
              "      <td>8</td>\n",
              "      <td>415</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>There's a package dedicated to statistics, see...</td>\n",
              "      <td>65729</td>\n",
              "      <td>&lt;computational_statistics&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2480</td>\n",
              "      <td>symbolic computer algebra statistics functiona...</td>\n",
              "      <td>0.122222</td>\n",
              "      <td>0.427778</td>\n",
              "      <td>0.273750</td>\n",
              "      <td>0.556944</td>\n",
              "      <td>1.304541e+09</td>\n",
              "      <td>symbolic computer algebra for statistics</td>\n",
              "      <td>&lt;computational_statistics&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2481</th>\n",
              "      <td>How to use rejection sampling to generate draw...</td>\n",
              "      <td>&lt;p&gt;I'm working on some practice test problems,...</td>\n",
              "      <td>&lt;self_study&gt;&lt;monte_carlo&gt;&lt;simulation&gt;</td>\n",
              "      <td>2011-05-04T21:17:49.260</td>\n",
              "      <td>2011-05-05T21:09:49.177</td>\n",
              "      <td>4</td>\n",
              "      <td>1114</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Wikipedia provides (without explanation) an [a...</td>\n",
              "      <td>65730</td>\n",
              "      <td>&lt;self_study&gt;</td>\n",
              "      <td>&lt;monte_carlo&gt;</td>\n",
              "      <td>&lt;simulation&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2481</td>\n",
              "      <td>use rejection sampling generate draws unit exp...</td>\n",
              "      <td>0.143229</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.283333</td>\n",
              "      <td>1.304544e+09</td>\n",
              "      <td>how to use rejection sampling to generate draw...</td>\n",
              "      <td>&lt;random_generation&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>226.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2482</th>\n",
              "      <td>$\\operatorname{Var}(X^2)$, if $\\operatorname{V...</td>\n",
              "      <td>&lt;p&gt;What would be &lt;span class=\"math-container\"&gt;...</td>\n",
              "      <td>&lt;mathematical_statistics&gt;&lt;variance&gt;</td>\n",
              "      <td>2011-05-05T03:42:06.087</td>\n",
              "      <td>2018-12-18T23:35:56.927</td>\n",
              "      <td>6</td>\n",
              "      <td>18479</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>$Var[X] \\stackrel{d}{=} \\mathbb{E}[X^2] - (\\ma...</td>\n",
              "      <td>65731</td>\n",
              "      <td>&lt;mathematical_statistics&gt;</td>\n",
              "      <td>&lt;variance&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2482</td>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>1.304567e+09</td>\n",
              "      <td>if</td>\n",
              "      <td>&lt;mathematical_statistics&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>236.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2483</th>\n",
              "      <td>Bootstrapping data envelopment analysis effici...</td>\n",
              "      <td>&lt;p&gt;I want to perform bootstrapping for calcula...</td>\n",
              "      <td>&lt;r&gt;&lt;bootstrap&gt;&lt;efficiency&gt;</td>\n",
              "      <td>2011-05-05T05:47:30.180</td>\n",
              "      <td>2012-10-09T09:41:04.573</td>\n",
              "      <td>3</td>\n",
              "      <td>1757</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>@user4472, please provide some context and mor...</td>\n",
              "      <td>65732</td>\n",
              "      <td>&lt;bootstrap&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2483</td>\n",
              "      <td>bootstrapping data envelopment analysis effici...</td>\n",
              "      <td>-0.125000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.283333</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>1.304574e+09</td>\n",
              "      <td>bootstrapping data envelopment analysis effici...</td>\n",
              "      <td>&lt;bootstrap&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>206.0</td>\n",
              "      <td>227.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2484</th>\n",
              "      <td>Panel Data: In a fixed effects model, does aut...</td>\n",
              "      <td>&lt;p&gt;Given a panel of countries over time, a fix...</td>\n",
              "      <td>&lt;autocorrelation&gt;&lt;panel_data&gt;&lt;fixed_effects_mo...</td>\n",
              "      <td>2011-05-05T08:06:46.827</td>\n",
              "      <td>2011-05-05T08:06:46.827</td>\n",
              "      <td>4</td>\n",
              "      <td>2855</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Wooldridge in his [book](http://books.google.c...</td>\n",
              "      <td>65733</td>\n",
              "      <td>&lt;autocorrelation&gt;</td>\n",
              "      <td>&lt;panel_data&gt;</td>\n",
              "      <td>&lt;fixed_effects_model&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2484</td>\n",
              "      <td>panel data fixed effects model auto correlatio...</td>\n",
              "      <td>0.126667</td>\n",
              "      <td>0.296667</td>\n",
              "      <td>0.104000</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>1.304583e+09</td>\n",
              "      <td>panel data in a fixed effects model does auto ...</td>\n",
              "      <td>&lt;panel_data&gt;</td>\n",
              "      <td>&lt;fixed_effects_model&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>242.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2485</th>\n",
              "      <td>When estimating variance, why do unbiased esti...</td>\n",
              "      <td>&lt;p&gt;I am totally confused: On the one hand you ...</td>\n",
              "      <td>&lt;normal_distribution&gt;&lt;variance&gt;&lt;unbiased_estim...</td>\n",
              "      <td>2011-05-05T08:11:02.280</td>\n",
              "      <td>2019-03-02T22:56:03.470</td>\n",
              "      <td>7</td>\n",
              "      <td>1831</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-</td>\n",
              "      <td>65734</td>\n",
              "      <td>&lt;normal_distribution&gt;</td>\n",
              "      <td>&lt;variance&gt;</td>\n",
              "      <td>&lt;unbiased_estimator&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2485</td>\n",
              "      <td>estimating variance unbiased estimators divide...</td>\n",
              "      <td>0.175463</td>\n",
              "      <td>0.541759</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.304583e+09</td>\n",
              "      <td>when estimating variance why do unbiased estim...</td>\n",
              "      <td>&lt;unbiased_estimator&gt;</td>\n",
              "      <td>&lt;maximum_likelihood&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>201.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2486</th>\n",
              "      <td>Is it problematic if one predictor in a set ac...</td>\n",
              "      <td>&lt;p&gt;I am running a logistic regression with cus...</td>\n",
              "      <td>&lt;logistic&gt;&lt;modeling&gt;</td>\n",
              "      <td>2011-05-05T09:34:46.440</td>\n",
              "      <td>2011-05-05T10:32:03.557</td>\n",
              "      <td>3</td>\n",
              "      <td>94</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>@ayush Could you edit your question clarifying...</td>\n",
              "      <td>65735</td>\n",
              "      <td>&lt;logistic&gt;</td>\n",
              "      <td>&lt;modeling&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2486</td>\n",
              "      <td>problematic one predictor set accounts almost ...</td>\n",
              "      <td>0.087326</td>\n",
              "      <td>0.450694</td>\n",
              "      <td>0.074578</td>\n",
              "      <td>0.515931</td>\n",
              "      <td>1.304588e+09</td>\n",
              "      <td>is it problematic if one predictor in a set ac...</td>\n",
              "      <td>&lt;predictor&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>42.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2487</th>\n",
              "      <td>Making a heatmap with a precomputed distance m...</td>\n",
              "      <td>&lt;p&gt;I have made a heatmap based upon a regular ...</td>\n",
              "      <td>&lt;r&gt;&lt;data_visualization&gt;</td>\n",
              "      <td>2011-05-05T09:39:26.173</td>\n",
              "      <td>2019-01-15T23:26:36.430</td>\n",
              "      <td>3</td>\n",
              "      <td>5130</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>@Lars What do you want to modify: the heatmap ...</td>\n",
              "      <td>65736</td>\n",
              "      <td>&lt;data_visualization&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2487</td>\n",
              "      <td>making heatmap precomputed distance matrix dat...</td>\n",
              "      <td>-0.041667</td>\n",
              "      <td>0.330128</td>\n",
              "      <td>0.360417</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>1.304588e+09</td>\n",
              "      <td>making a heatmap with a precomputed distance m...</td>\n",
              "      <td>&lt;matrix&gt;</td>\n",
              "      <td>&lt;data_visualization&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>31.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>Advice on missing value imputation</td>\n",
              "      <td>&lt;p&gt;I am working on insurance data in which a c...</td>\n",
              "      <td>&lt;data_imputation&gt;</td>\n",
              "      <td>2011-05-05T11:20:13.343</td>\n",
              "      <td>2012-12-19T20:10:02.963</td>\n",
              "      <td>5</td>\n",
              "      <td>646</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>In my opinion, if the variable is significant ...</td>\n",
              "      <td>65737</td>\n",
              "      <td>&lt;data_imputation&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2488</td>\n",
              "      <td>advice missing value imputation working insura...</td>\n",
              "      <td>0.207213</td>\n",
              "      <td>0.510014</td>\n",
              "      <td>0.070455</td>\n",
              "      <td>0.513182</td>\n",
              "      <td>1.304594e+09</td>\n",
              "      <td>advice on missing value imputation</td>\n",
              "      <td>&lt;missing_data&gt;</td>\n",
              "      <td>&lt;data_imputation&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>207.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2489</th>\n",
              "      <td>How to analyse repeated measure ANOVA with thr...</td>\n",
              "      <td>&lt;h3&gt;Context:&lt;/h3&gt;\\n\\n&lt;p&gt;My question concerns a...</td>\n",
              "      <td>&lt;hypothesis_testing&gt;&lt;anova&gt;&lt;repeated_measures&gt;</td>\n",
              "      <td>2011-05-05T12:26:43.317</td>\n",
              "      <td>2014-12-06T05:40:54.650</td>\n",
              "      <td>8</td>\n",
              "      <td>1730</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>Could you specify what your question is? Is th...</td>\n",
              "      <td>65738</td>\n",
              "      <td>&lt;hypothesis_testing&gt;</td>\n",
              "      <td>&lt;anova&gt;</td>\n",
              "      <td>&lt;repeated_measures&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>2489</td>\n",
              "      <td>analyse repeated measure anova three condition...</td>\n",
              "      <td>0.011835</td>\n",
              "      <td>0.460504</td>\n",
              "      <td>-0.014794</td>\n",
              "      <td>0.437511</td>\n",
              "      <td>1.304598e+09</td>\n",
              "      <td>how to analyse repeated measure anova with thr...</td>\n",
              "      <td>&lt;repeated_measures&gt;</td>\n",
              "      <td>&lt;anova&gt;</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>244.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  ... topic_pred4\n",
              "2480           Symbolic computer algebra for statistics  ...      1000.0\n",
              "2481  How to use rejection sampling to generate draw...  ...      1000.0\n",
              "2482  $\\operatorname{Var}(X^2)$, if $\\operatorname{V...  ...      1000.0\n",
              "2483  Bootstrapping data envelopment analysis effici...  ...      1000.0\n",
              "2484  Panel Data: In a fixed effects model, does aut...  ...      1000.0\n",
              "2485  When estimating variance, why do unbiased esti...  ...      1000.0\n",
              "2486  Is it problematic if one predictor in a set ac...  ...      1000.0\n",
              "2487  Making a heatmap with a precomputed distance m...  ...      1000.0\n",
              "2488                 Advice on missing value imputation  ...      1000.0\n",
              "2489  How to analyse repeated measure ANOVA with thr...  ...      1000.0\n",
              "\n",
              "[10 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDLDn4VjhXfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}